{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.data_process import load_keel_dataset, keel_dataset_preprocess, get_keel_dataset_batch_list, batch_list_to_df\n",
    "# from utils.train import init_net, init_optimizers, train\n",
    "# from utils.generate_new_samples import generate_new_1_sample\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "\n",
    "# 设置GPU相关信息\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"  # 指定哪块GPU训练\n",
    "# config = tf.compat.v1.ConfigProto()\n",
    "# # 设置最大占有GPU不超过显存的80%（可选）\n",
    "# # config.gpu_options.per_process_gpu_memory_fraction=0.8\n",
    "# config.gpu_options.allow_growth = True  # 设置动态分配GPU内存\n",
    "# sess = tf.compat.v1.Session(config=config)\n",
    "\n",
    "\n",
    "# 初始化编码维度 ：enc_dim，即z的维度\n",
    "enc_dim = 64\n",
    "# 指定epochs\n",
    "epochs = 300\n",
    "# 指定batch_size\n",
    "batch_size = 16\n",
    "# 指定vae损失计算的参数alpha_1和alpha_2\n",
    "vae_loss_paramaters_dict = {\"alpha_1\": 1.5, \"alpha_2\": 0.1}\n",
    "# 设置数据库文件路径以及性能指标保存路径\n",
    "project_path = './'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "数据处理\n",
    "    加载keel数据集数据\n",
    "    数据预处理\n",
    "    根据交叉验证获取训练集和测试机batch\n",
    "    将一个batch转为array, df\n",
    "    将一个batch_list转为array, df\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "def load_keel_dataset(dataset_path, keel_dataset_name):\n",
    "    \"\"\"\n",
    "    加载keel数据集\n",
    "    先获取keel数据集的特征和具体数据，依据此来构造df\n",
    "\n",
    "    :param dataset_path : 数据集路径\n",
    "    :param keel_dataset_name : 需要加载的keel数据集名称\n",
    "    :return df : 返回加载的数据集的pandas.DataFrame数据格式\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(dataset_path + keel_dataset_name)\n",
    "    # 字符串转为数字，忽略错误（默认返回dtype为float64或int64，具体取决于提供的数据。）\n",
    "    df = df.apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def keel_dataset_preprocess(keel_dataset_df):\n",
    "    \"\"\"\n",
    "    对pandas.DataFrame格式的keel_dataset进行预处理\n",
    "    主要包括：\n",
    "            对样本标签的操作：将多数类样本的标签赋值为0，少数类样本的标签赋值为1\n",
    "            对样本属性的操作：进行特征提取，将非数字属性值转换为one-hot编码格式\n",
    "                           将数据归一化到(-1, 1)范围内\n",
    "    \n",
    "    :param : keel_dataset_df : keel数据集经过数据集加载后得到的pandas.DataFrame格式\n",
    "\n",
    "    :return : keel_dataset_df : 经过上述处理后的keel_dataset_df\n",
    "    \"\"\"\n",
    "    # 添加一列权重的部分， 权重=10个近邻中   1.0 * (多数类样本的个数 + 1) / (少数类样本的个数 + 1)\n",
    "    knn = KNeighborsClassifier(n_neighbors=10)\n",
    "    knn.fit(np.array(keel_dataset_df.iloc[:, :-1]), np.array(keel_dataset_df.iloc[:, -1]))\n",
    "    label_sum_list = []\n",
    "    for i in range(keel_dataset_df.shape[0]):\n",
    "        knn_index = knn.kneighbors([(keel_dataset_df.iloc[i, :-1])], return_distance=False)\n",
    "        label_sum = keel_dataset_df.iloc[knn_index[0], -1].sum()\n",
    "        label_sum_list.append(1.0 * (10 - label_sum + 1) / (label_sum + 1))\n",
    "    keel_dataset_df.insert(loc=len(keel_dataset_df.columns) - 1, column='knn', value=label_sum_list)\n",
    "    \n",
    "    return keel_dataset_df\n",
    "\n",
    "\n",
    "def get_keel_dataset_batch_list(keel_dataset_df, batch_size=16, my_random_state=2022):\n",
    "    \"\"\"\n",
    "    根据n折交叉验证和batch_size大小 获取经过预处理后keel数据集的batch_list\n",
    "\n",
    "    :param : keel_dataset_df : 经过预处理后的keel数据集，数据类型为pandas.DataFrame\n",
    "    :param : my_n_splits : 交叉验证参数，要分割为多少个子集\n",
    "    :param : batch_size : batch大小\n",
    "    :param : my_random_state : random_state大小\n",
    "\n",
    "    :return : batch_list : [train_batch_list, val_batch_list]\n",
    "    \"\"\"\n",
    "    # 初始化train_batch_list, val_batch_list\n",
    "    train_batch_list, val_batch_list = [], []\n",
    "\n",
    "    # 获取多数类样本和少数类样本的df并进行打乱\n",
    "    data_0_df = keel_dataset_df[keel_dataset_df[keel_dataset_df.columns[-1]] == 0]\n",
    "    data_1_df = keel_dataset_df[keel_dataset_df[keel_dataset_df.columns[-1]] == 1]\n",
    "    data_0_df.index = range(data_0_df.shape[0])\n",
    "    data_1_df.index = range(data_1_df.shape[0])\n",
    "        \n",
    "    class_0_train_index = list(data_0_df.index)\n",
    "    class_1_train_index = list(data_1_df.index)\n",
    "\n",
    "    class_0_val_index = random.sample(class_0_train_index, (int)(data_0_df.shape[0] / 4))\n",
    "    class_1_val_index = random.sample(class_1_train_index, (int)(data_1_df.shape[0] / 4))\n",
    "\n",
    "    class_0_train_index = list(set(class_0_train_index).difference(class_0_val_index))\n",
    "    class_1_train_index = list(set(class_1_train_index).difference(class_1_val_index))\n",
    "\n",
    "    # 根据上述index列表，获取一次交叉验证中所有的训练集样本\n",
    "    train_dataset_0_df = data_0_df.iloc[class_0_train_index]\n",
    "    train_dataset_1_df = data_1_df.iloc[class_1_train_index]\n",
    "\n",
    "    # 根据上述index列表，获取一次交叉验证中所有的验证集样本\n",
    "    val_dataset_0_df = data_0_df.iloc[class_0_val_index]\n",
    "    val_dataset_1_df = data_1_df.iloc[class_1_val_index]\n",
    "\n",
    "    train_dataset_df = pd.concat([train_dataset_0_df, train_dataset_1_df], axis=0)\n",
    "    val_dataset_df = pd.concat([val_dataset_0_df, val_dataset_1_df], axis=0)\n",
    "    \n",
    "    # 获取 训练集中 多数类样本与少数类样本的比例 rate = 多数类样本数目 / 少数类样本数目\n",
    "    rate = (len(class_0_train_index) // len(class_1_train_index)) + 1\n",
    "    \n",
    "    # 对batch_size大小进行调整，如果 训练集中少数类样本的数目 大于 原batch_size 的四倍，则不变，否则调整后的batch_size是训练集中少数类样本的数目大小的1/4\n",
    "    if batch_size * 4 > len(class_1_train_index):\n",
    "        batch_size = (len(class_1_train_index) // 4) + 1\n",
    "    \n",
    "    # 多数类样本和少数类类样本使用不同的batch_size，多数类类样本的batch_size = 调整后的batch_size（少数类类样本的batch_size）* rate\n",
    "    train_batch_list = [tf.data.Dataset.from_tensor_slices((train_dataset_0_df.iloc[:, :-1].values, train_dataset_0_df.iloc[:, -1].values)).batch(batch_size*rate).shuffle(100), \n",
    "                            tf.data.Dataset.from_tensor_slices((train_dataset_1_df.iloc[:, :-1].values, train_dataset_1_df.iloc[:, -1].values)).batch(batch_size).shuffle(100)]\n",
    "    \n",
    "    \n",
    "    val_batch_list = tf.data.Dataset.from_tensor_slices((val_dataset_df.iloc[:, :-1].values, val_dataset_df.iloc[:, -1].values)).batch(batch_size).shuffle(100)\n",
    "\n",
    "    batch_list = [train_batch_list, val_batch_list]\n",
    "    \n",
    "    return batch_list\n",
    "\n",
    "\n",
    "def batch_to_array(one_batch):\n",
    "    \"\"\"\n",
    "    将一个batch转为np.array\n",
    "    \"\"\"\n",
    "    # 分别获取属性值与标签值的array\n",
    "    attribute_value_array = np.array(one_batch[0])\n",
    "    label_array = np.transpose(np.array(one_batch[1]))\n",
    "    label_array = label_array[:, np.newaxis]  # 升维  (n, ) ----> (n, 1)\n",
    "    # 合并np.array\n",
    "    one_batch_array = np.concatenate([attribute_value_array, label_array], axis=1)\n",
    "        \n",
    "    return one_batch_array\n",
    "\n",
    "\n",
    "def batch_list_to_array(one_batch_list):\n",
    "    \"\"\"\n",
    "    将 [batch_1, batch_2, ..., batch_n, ...] 转为array\n",
    "    \"\"\"\n",
    "    one_batch_array_list = []\n",
    "    # 获取每一个batch的array\n",
    "    for one_batch in one_batch_list:\n",
    "        one_batch_array = batch_to_array(one_batch=one_batch)\n",
    "        one_batch_array_list.append(one_batch_array)\n",
    "    # 纵向合并\n",
    "    one_batch_list_array = np.concatenate(one_batch_array_list, axis=0)\n",
    "    return one_batch_list_array\n",
    "\n",
    "\n",
    "def batch_to_df(one_batch, columns):\n",
    "    \"\"\"\n",
    "    将一个batch转为df\n",
    "    \"\"\"\n",
    "    # 先将batch转为np.array\n",
    "    one_batch_array = batch_to_array(one_batch=one_batch)\n",
    "    # 将np.array转为df\n",
    "    df = pd.DataFrame(one_batch_array, columns=columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def batch_list_to_df(one_batch_list, columns):\n",
    "    \"\"\"\n",
    "    将 [batch_1, batch_2, ..., batch_n, ...] 转为df\n",
    "    \"\"\"\n",
    "\n",
    "    # 将one_batch_list转为np.array\n",
    "    one_batch_list_array = batch_list_to_array(one_batch_list=one_batch_list)\n",
    "\n",
    "    # 将np.array转为df\n",
    "    df = pd.DataFrame(one_batch_list_array, columns=columns) \n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def select_1_samples_by_lr(data_0_df, data_1_df, migrate_data_1_df):\n",
    "    original_data = pd.concat([data_0_df, data_1_df], axis=0)\n",
    "    train_x = original_data.iloc[:, :-1]\n",
    "    train_y = original_data.iloc[:, -1]\n",
    "    test_x = migrate_data_1_df.iloc[:, :-1]\n",
    "\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(train_x, train_y)\n",
    "    test_x = test_x.fillna(0)\n",
    "    proba_array = lr.predict_proba(test_x)[:, -1]\n",
    "    migrate_data_1_df['proba'] = proba_array\n",
    "    migrate_data_1_df.sort_values(by=\"proba\", inplace=True, ascending=True)\n",
    "    migrate_data_1_df.index = range(len(migrate_data_1_df))\n",
    "    m = data_0_df.shape[0] - data_1_df.shape[0]\n",
    "    migrate_data_1_df = migrate_data_1_df.iloc[[i for i in range(m)], :]\n",
    "    # 再将最后一列删除\n",
    "    selected_migrate_data_1_df = migrate_data_1_df.drop(columns='proba')\n",
    "\n",
    "    return selected_migrate_data_1_df\n",
    "\n",
    "\n",
    "def select_1_samples_by_rf(data_0_df, data_1_df, migrate_data_1_df):\n",
    "    original_data = pd.concat([data_0_df, data_1_df], axis=0)\n",
    "    train_x = original_data.iloc[:, :-1]\n",
    "    train_y = original_data.iloc[:, -1]\n",
    "    test_x = migrate_data_1_df.iloc[:, :-1]\n",
    "\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(train_x, train_y)\n",
    "    test_x = test_x.fillna(0)\n",
    "    proba_array = rf.predict_proba(test_x)[:, -1]\n",
    "    migrate_data_1_df['proba'] = proba_array\n",
    "    migrate_data_1_df.sort_values(by=\"proba\", inplace=True, ascending=True)\n",
    "    migrate_data_1_df.index = range(len(migrate_data_1_df))\n",
    "    m = data_0_df.shape[0] - data_1_df.shape[0]\n",
    "    migrate_data_1_df = migrate_data_1_df.iloc[[i for i in range(m)], :]\n",
    "    # 再将最后一列删除\n",
    "    selected_migrate_data_1_df = migrate_data_1_df.drop(columns='proba')\n",
    "\n",
    "    return selected_migrate_data_1_df\n",
    "\n",
    "\n",
    "def select_1_samples_by_svm(data_0_df, data_1_df, migrate_data_1_df):\n",
    "    original_data = pd.concat([data_0_df, data_1_df], axis=0)\n",
    "    train_x = original_data.iloc[:, :-1]\n",
    "    train_y = original_data.iloc[:, -1]\n",
    "    test_x = migrate_data_1_df.iloc[:, :-1]\n",
    "\n",
    "    svm = SVC(probability=True)\n",
    "    svm.fit(train_x, train_y)\n",
    "    test_x = test_x.fillna(0)\n",
    "    proba_array = svm.predict_proba(test_x)[:, -1]\n",
    "    migrate_data_1_df['proba'] = proba_array\n",
    "    migrate_data_1_df.sort_values(by=\"proba\", inplace=True, ascending=True)\n",
    "    migrate_data_1_df.index = range(len(migrate_data_1_df))\n",
    "    m = data_0_df.shape[0] - data_1_df.shape[0]\n",
    "    migrate_data_1_df = migrate_data_1_df.iloc[[i for i in range(m)], :]\n",
    "    # 再将最后一列删除\n",
    "    selected_migrate_data_1_df = migrate_data_1_df.drop(columns='proba')\n",
    "\n",
    "    return selected_migrate_data_1_df\n",
    "\n",
    "\n",
    "def generate_new_1_sample(optimized_smsg_net_dict, train_batch, columns):\n",
    "    \"\"\"\n",
    "    从优化好的网络中生成新的少数类样本，将其添加到原来的训练集中\n",
    "    :param : optimized_smsg_net_dict : 优化好的SMSG_PRO网络\n",
    "    :param : train_batch : 训练集batches，包括多数类样本和少数类样本的batches\n",
    "    :param : columns : df的列名\n",
    "\n",
    "    :return : balanced_train_dataset_df : 经过平衡后的训练数据集batch\n",
    "    \"\"\"\n",
    "\n",
    "    # 取出优化好的SMSG网络的编码器0和解码器1\n",
    "    enc = optimized_smsg_net_dict.get('enc')\n",
    "    dec = optimized_smsg_net_dict.get('dec')\n",
    "    map_net = optimized_smsg_net_dict.get('map_net')\n",
    "\n",
    "    # 初始化需要添加到训练集的迁移少数类batch_list\n",
    "    migrate_data_1_batch_list = []\n",
    "\n",
    "    # 取出每一个训练data_0_batch\n",
    "    for data_0_batch in train_batch[0]:\n",
    "        x_0, y_0 = data_0_batch\n",
    "        # 类型转换，将x_0中数据类型转为tf.float32\n",
    "        x_0 = tf.cast(x_0, dtype=tf.float32)\n",
    "        mean_and_stddec_0 = enc(x_0)  # 使用优化好的编码器对多数类样本进行编码\n",
    "\n",
    "        # 多数类样本迁移得到迁移少数类样本\n",
    "        mean_and_stddec_0_1 = map_net(mean_and_stddec_0)  # 多数类样本的编码经过映射网络得到少数类样本的迁移编码\n",
    "        mean_0_1, stddec_0_1 = tf.split(mean_and_stddec_0_1, 2, 1)  # 获取分布的两个参数\n",
    "        z_0_1 = sample_z(mean_0_1, stddec_0_1)  # 期望与z_1一致\n",
    "        x_0_1 = dec(z_0_1)  # 使用迁移编码经过解码器得到的迁移少数类样本\n",
    "        y_0_1 = tf.constant(1.0, shape=y_0.shape)  # 对标签进行赋值1.0\n",
    "        added_data_1_batch = (x_0_1, y_0_1)\n",
    "        # 将每一次生成的样本batch添加到列表中\n",
    "        migrate_data_1_batch_list.append(added_data_1_batch)\n",
    "\n",
    "    # 得到原始多数类，原始少数类，迁移少数类的df\n",
    "    data_0_df = batch_list_to_df(train_batch[0], columns=columns)\n",
    "    data_1_df = batch_list_to_df(train_batch[1], columns=columns)\n",
    "\n",
    "    migrate_data_1_df = batch_list_to_df(migrate_data_1_batch_list, columns=columns)\n",
    "    # 进行一些深拷贝\n",
    "    migrate_data_1_df2 = migrate_data_1_df.copy(deep=True)\n",
    "    migrate_data_1_df3 = migrate_data_1_df.copy(deep=True)\n",
    "    data_0_df2 = data_0_df.copy(deep=True)\n",
    "    data_0_df3 = data_0_df.copy(deep=True)\n",
    "    data_1_df2 = data_1_df.copy(deep=True)\n",
    "    data_1_df3 = data_1_df.copy(deep=True)\n",
    "\n",
    "    # data_0_df.to_csv('/home/lqw/testone/ttgan/ttgan/temp/data_0_df.csv', index=False, header=True, sep=',')\n",
    "    # data_1_df.to_csv('/home/lqw/testone/ttgan/ttgan/temp/data_1_df.csv', index=False, header=True, sep=',')\n",
    "    # migrate_data_1_df.to_csv('/home/lqw/testone/ttgan/ttgan/temp/migrate_data_1_df.csv', index=False, header=True, sep=',')\n",
    "\n",
    "    # 从迁移生成的少数类样本的df中随机抽取原始多数类和原始少数类的差值个样本，保证生成后总的多数类和少数类样本数目相同\n",
    "    # migrate_data_1_df = migrate_data_1_df.sample(n=data_0_df.shape[0]-data_1_df.shape[0], random_state=2022)\n",
    "\n",
    "    migrate_data_1_df_by_lr = select_1_samples_by_lr(data_0_df, data_1_df, migrate_data_1_df)\n",
    "    balanced_train_dataset_df_by_lr = pd.concat([data_0_df, data_1_df, migrate_data_1_df_by_lr], axis=0)\n",
    "\n",
    "    migrate_data_1_df_by_rf = select_1_samples_by_rf(data_0_df2, data_1_df2, migrate_data_1_df2)\n",
    "    balanced_train_dataset_df_by_rf = pd.concat([data_0_df, data_1_df, migrate_data_1_df_by_rf], axis=0)\n",
    "\n",
    "    migrate_data_1_df_by_svm = select_1_samples_by_svm(data_0_df3, data_1_df3, migrate_data_1_df3)\n",
    "    balanced_train_dataset_df_by_svm = pd.concat([data_0_df, data_1_df, migrate_data_1_df_by_svm], axis=0)\n",
    "\n",
    "    return {'lr': balanced_train_dataset_df_by_lr, 'rf': balanced_train_dataset_df_by_rf,\n",
    "            'svm': balanced_train_dataset_df_by_svm}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "smsg_pro_pro训练\n",
    "\"\"\"\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import recall_score, f1_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def init_net(x_dim, enc_dim):\n",
    "    \"\"\"\n",
    "    初始化SMSG_PRO的网络结构\n",
    "    enc    dec\n",
    "    map_net\n",
    "    dis\n",
    "\n",
    "    :param : x_dim : 样本的维度\n",
    "    :param : enc_dim : 编码器编码的维度\n",
    "\n",
    "    :return : smsg_net_dict = {'enc':enc, 'dec':dec, 'map_net':map_net, 'dis':dis}\n",
    "    \"\"\"\n",
    "    enc = Encoder(enc_dim)  # 编码器\n",
    "    dec = Decoder(x_dim)  # 解码器\n",
    "    map_net = Map_Net(enc_dim)  # 映射网络\n",
    "    dis = Discriminator(x_dim)  # 对样本进行判别的判别器\n",
    "    dis_map = Discriminator(x_dim=enc_dim)  # 对隐编码进行判别的判别器\n",
    "\n",
    "    smsg_net_dict = {'enc': enc, 'dec': dec, 'map_net': map_net, 'dis': dis, 'dis_map': dis_map}\n",
    "    return smsg_net_dict\n",
    "\n",
    "\n",
    "def init_optimizers(learning_rate=2e-4, beta_1=0.5):\n",
    "    \"\"\"\n",
    "    初始化上述网络的优化器\n",
    "    网络优化策略设置，优化器为Adam(学习率为2e-4，beta_1为0.5)\n",
    "    :param : learning_rate default = 2e-4\n",
    "    :param : beta_1 default = 0.5\n",
    "\n",
    "    :return : optimizers_dict = {'optimizer_enc':optimizer_enc, 'optimizer_dec':optimizer_dec,\n",
    "                                 'optimizer_map_net':optimizer_map_net, 'optimizer_dis':optimizer_dis, }\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    optimizer_enc = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1)\n",
    "    optimizer_dec = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1)\n",
    "    optimizer_map_net = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1)\n",
    "    optimizer_dis = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1)\n",
    "    optimizer_dis_map = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1)\n",
    "\n",
    "    optimizers_dict = {'optimizer_enc': optimizer_enc, 'optimizer_dec': optimizer_dec,\n",
    "                       'optimizer_map_net': optimizer_map_net, 'optimizer_dis': optimizer_dis,\n",
    "                       'optimizer_dis_map': optimizer_dis_map}\n",
    "\n",
    "    return optimizers_dict\n",
    "    \n",
    "def batch_list_to_df(one_batch_list, columns):\n",
    "    \"\"\"\n",
    "    将 [batch_1, batch_2, ..., batch_n, ...] 转为df\n",
    "    \"\"\"\n",
    "\n",
    "    # 将one_batch_list转为np.array\n",
    "    one_batch_list_array = batch_list_to_array(one_batch_list=one_batch_list)\n",
    "\n",
    "    # 将np.array转为df\n",
    "    df = pd.DataFrame(one_batch_list_array, columns=columns) \n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self, x_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.x_dim = x_dim\n",
    "        self.Line = tf.keras.Sequential([\n",
    "            # 隐层\n",
    "            layers.Dense(self.x_dim * 2, kernel_initializer='glorot_uniform'),\n",
    "            layers.LeakyReLU(0.2),\n",
    "            layers.Dropout(0.5),\n",
    "            \n",
    "            layers.Dense(256*16, kernel_initializer='glorot_uniform'),\n",
    "            layers.LeakyReLU(0.2),\n",
    "            layers.Dropout(0.5),\n",
    "\n",
    "            layers.Dense(128*8, kernel_initializer='glorot_uniform'),\n",
    "            layers.LeakyReLU(0.2),\n",
    "            layers.Dropout(0.5),\n",
    "\n",
    "            layers.Dense(64*4, kernel_initializer='glorot_uniform'),\n",
    "            layers.LeakyReLU(0.2),\n",
    "            layers.Dropout(0.5),\n",
    "\n",
    "            layers.Dense(64, kernel_initializer='glorot_uniform'),\n",
    "            layers.LeakyReLU(0.2),\n",
    "            layers.Dropout(0.5),\n",
    "            \n",
    "            # 输出层\n",
    "            layers.Dense(1, kernel_initializer='glorot_uniform'),\n",
    "            layers.Activation('sigmoid')\n",
    "        ])\n",
    "\n",
    "    def call(self, x, is_training=1):\n",
    "        # 判别网络的输出\n",
    "        out = self.Line(x, training=is_training)\n",
    "\n",
    "        return out\n",
    "\n",
    "def sample_z(mean, stddev):\n",
    "    \"\"\"\n",
    "    根据均值和标准差进行采样得到z\n",
    "    具体做法：先得到z的标准正态分布，再根据均值和标准差进行变换得到z\n",
    "    :param : mean : 均值\n",
    "    :param : stddev : 标准差\n",
    "    \"\"\"\n",
    "    # # 标准正态分布生成器\n",
    "    # std_normal_distribution_init = tf.random_normal_initializer(stddev=1.0) \n",
    "    # # 得到z的标准正态分布std_z\n",
    "    # std_z = std_normal_distribution_init(shape=mean.shape)\n",
    "    \n",
    "    # # 变换 \n",
    "    # z = mean + std_z * stddev\n",
    "\n",
    "    # return z\n",
    "\n",
    "    eps_init = tf.random_normal_initializer()\n",
    "    eps = eps_init(shape=mean.shape)\n",
    "\n",
    "    return mean + eps * tf.exp(stddev)\n",
    "\n",
    "def get_svm_performance_dict(train_datatset_df, test_datatset_df):\n",
    "    \"\"\"\n",
    "    获取平衡后的数据集在SVM上的表现\n",
    "    :param : train_datatset_df : 平衡后的训练集\n",
    "    :param : test_datatset_df : 原测试集\n",
    "\n",
    "    :return : {'method_name': 'svm', 'f1':my_f1, 'gmean':my_gmean}\n",
    "    \"\"\"\n",
    "    \n",
    "    train_x = np.array(train_datatset_df.iloc[:, :-1])\n",
    "    train_y = np.array(train_datatset_df.iloc[:, -1])\n",
    "    test_x = np.array(test_datatset_df.iloc[:, :-1])\n",
    "    test_y = np.array(test_datatset_df.iloc[:, -1])\n",
    "    # 初始化SVM\n",
    "    svm = SVC()\n",
    "    svm.fit(train_x, train_y)\n",
    "    predicted_y = svm.predict(test_x)\n",
    "    # 获取性能\n",
    "    my_f1 = f1_score(test_y, predicted_y, average=None)[1]\n",
    "    recall = recall_score(test_y, predicted_y, average=None)\n",
    "    my_gmean = math.sqrt(recall[0] * recall[1])\n",
    "    return {'method_name': 'svm', 'f1':my_f1, 'gmean':my_gmean}\n",
    "\n",
    "\n",
    "def get_lr_performance_dict(train_datatset_df, test_datatset_df):\n",
    "    \"\"\"\n",
    "    获取平衡后的数据集在LR上的表现\n",
    "    :param : train_datatset_df : 平衡后的训练集\n",
    "    :param : test_datatset_df : 原测试集\n",
    "\n",
    "    :return : {'method_name': 'lr', 'f1':my_f1, 'gmean':my_gmean}\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    train_x = np.array(train_datatset_df.iloc[:, :-1])\n",
    "    train_y = np.array(train_datatset_df.iloc[:, -1])\n",
    "    test_x = np.array(test_datatset_df.iloc[:, :-1])\n",
    "    test_y = np.array(test_datatset_df.iloc[:, -1])\n",
    "\n",
    "    # 初始化lr\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(train_x, train_y)\n",
    "    predicted_y = lr.predict(test_x)\n",
    "    # 获取性能\n",
    "    my_f1 = f1_score(test_y, predicted_y, average=None)[1]  # index=0得到的是类0的F1，index=1得到的是类1的F1，这里我们需要的是类1（少数类）的F1\n",
    "    recall = recall_score(test_y, predicted_y, average=None)\n",
    "    my_gmean = math.sqrt(recall[0] * recall[1])\n",
    "    return {'method_name': 'lr', 'f1':my_f1, 'gmean':my_gmean}\n",
    "\n",
    "\n",
    "def get_rf_performance_dict(train_datatset_df, test_datatset_df):\n",
    "    \"\"\"\n",
    "    获取平衡后的数据集在RF上的表现\n",
    "    :param : train_datatset_df : 平衡后的训练集\n",
    "    :param : test_datatset_df : 原测试集\n",
    "\n",
    "    :return : {'method_name': 'rf', 'f1':my_f1, 'gmean':my_gmean}\n",
    "    \"\"\"\n",
    "    \n",
    "    train_x = np.array(train_datatset_df.iloc[:, :-1])\n",
    "    train_y = np.array(train_datatset_df.iloc[:, -1])\n",
    "    test_x = np.array(test_datatset_df.iloc[:, :-1])\n",
    "    test_y = np.array(test_datatset_df.iloc[:, -1])\n",
    "\n",
    "    # 初始化lr\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(train_x, train_y)\n",
    "    predicted_y = rf.predict(test_x)\n",
    "    # 获取性能\n",
    "    my_f1 = f1_score(test_y, predicted_y, average=None)[1]\n",
    "    recall = recall_score(test_y, predicted_y, average=None)\n",
    "    my_gmean = math.sqrt(recall[0] * recall[1])\n",
    "    return {'method_name': 'rf', 'f1':my_f1, 'gmean':my_gmean}\n",
    "\n",
    "\n",
    "def get_three_methods_performance_df(train_datatset_df_dict, test_datatset_df):\n",
    "    \"\"\"\n",
    "    综合上述三个方法，将上述三个方法得到的字典合成为一个pandas.DataFrame\n",
    "    :param : train_datatset_df_dict : 平衡后的训练集字典\n",
    "    :param : test_datatset_df : 原测试集\n",
    "    \n",
    "    :return : three_methods_performance_df\n",
    "               method       f1          gmean\n",
    "                svm      f1-value     gmean-value\n",
    "                lr       f1-value     gmean-value\n",
    "                rf       f1-value     gmean-value\n",
    "    \"\"\"\n",
    "    train_datatset_df_lr = train_datatset_df_dict.get('lr')\n",
    "    train_datatset_df_rf = train_datatset_df_dict.get('rf')\n",
    "    train_datatset_df_svm = train_datatset_df_dict.get('svm')\n",
    "\n",
    "    svm_performance_dict = get_svm_performance_dict(train_datatset_df_svm, test_datatset_df)\n",
    "    lr_performance_dict = get_lr_performance_dict(train_datatset_df_lr, test_datatset_df)\n",
    "    rf_performance_dict = get_rf_performance_dict(train_datatset_df_rf, test_datatset_df)\n",
    "\n",
    "    method_list = [svm_performance_dict.get('method_name'), lr_performance_dict.get('method_name'), rf_performance_dict.get('method_name')]\n",
    "    lr_list = [svm_performance_dict.get('f1'), lr_performance_dict.get('f1'), rf_performance_dict.get('f1')]\n",
    "    rf_list = [svm_performance_dict.get('gmean'), lr_performance_dict.get('gmean'), rf_performance_dict.get('gmean')]\n",
    "    \n",
    "    three_methods_performance_df = pd.DataFrame(data=np.transpose([method_list, lr_list, rf_list]), columns=['method', 'f1', 'gmean'])\n",
    "\n",
    "    return three_methods_performance_df\n",
    "\n",
    "class Map_Net(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    映射网络\n",
    "\n",
    "    把多数类样本的隐编码映射为少数类样本的隐编码\n",
    "    \"\"\"\n",
    "    def __init__(self, enc_dim):\n",
    "        self.enc_dim = enc_dim * 2\n",
    "        super(Map_Net, self).__init__()\n",
    "        self.Map_Net0 = tf.keras.Sequential([\n",
    "            \n",
    "            keras.layers.Dense(units = 1024, kernel_initializer = 'uniform', activation = 'elu',\n",
    "                kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "            keras.layers.Dropout(0.5),  # 随机失活，有利于防止过拟合\n",
    "\n",
    "            keras.layers.Dense(units = 512, kernel_initializer = 'uniform', activation = 'elu',\n",
    "                kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "            keras.layers.Dropout(0.5),  # 随机失活，有利于防止过拟合\n",
    "\n",
    "            keras.layers.Dense(units = 256, kernel_initializer = 'uniform', activation = 'elu',\n",
    "                kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "            keras.layers.Dropout(0.5),  # 随机失活，有利于防止过拟合\n",
    "\n",
    "            layers.Dense(self.enc_dim),  # 全连接层，units=128是输出节点数\n",
    "            layers.Dropout(0.5),  # 随机失活，有利于防止过拟合\n",
    "            layers.LeakyReLU(0.2)  # 使用LeakyReLU作为激活函数\n",
    "        ])\n",
    "        self.Map_Net1 = tf.keras.Sequential([\n",
    "            layers.Dense(self.enc_dim, activation='tanh'),\n",
    "        ])\n",
    "    def call(self, input_enc, is_training=1):\n",
    "        \"\"\"\n",
    "        :param : input_enc : 输入的多数类样本的编码\n",
    "        :return : output_enc : 得到的少数类样本的编码\n",
    "        \"\"\"\n",
    "        temp = self.Map_Net0(input_enc, training=is_training)\n",
    "        output_enc = self.Map_Net1(temp, training=is_training)\n",
    "\n",
    "        return output_enc\n",
    "\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, enc_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_dim = enc_dim * 2\n",
    "        # Sequential()方法是一个容器，描述了神经网络的网络结构，在Sequential()的输入参数中描述从输入层到输出层的网络结构\n",
    "        self.Encoder0 = tf.keras.Sequential([\n",
    "            layers.Reshape((22, 8, 30)),  # 形状重塑层，将512重塑为128*4\n",
    "            # input= layers.Input(shape=(22, 8, 30))\n",
    "            layers.Conv2D(16, (2, 2), strides=(1, 1), name='conv1', input_shape = (22, 8, 30)),  # 也可以写为Conv2D(64, 2, strides=(1, 1), name='convl)\n",
    "            # keras.layers.Conv2D(128, 4),  # 一维卷积层，输出空间维度为128，卷积核大小为3，保持输入输出尺寸相同\n",
    "\n",
    "            layers.Flatten(),  # 压平层，把多维的输入一维化，常用在从卷积层到全连接层的过渡\n",
    "            \n",
    "            layers.Dense(units = 1024, kernel_initializer = 'uniform', activation = 'elu',\n",
    "            kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "            layers.Dropout(0.5),  # 随机失活，有利于防止过拟合\n",
    "\n",
    "            layers.Dense(units = 256, kernel_initializer = 'uniform', activation = 'elu',\n",
    "            kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "            layers.Dropout(0.5),  # 随机失活，有利于防止过拟合\n",
    "\n",
    "            layers.Dense(units = 128, kernel_initializer = 'uniform', activation = 'elu',\n",
    "            kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "            layers.Dropout(0.5),  # 随机失活，有利于防止过拟合\n",
    "            layers.LeakyReLU(0.2)  # 使用LeakyReLU作为激活函数\n",
    "        ])\n",
    "        self.Encoder1 = tf.keras.Sequential([\n",
    "            layers.Dense(self.enc_dim, activation='tanh'),\n",
    "        ])\n",
    "\n",
    "    # 具体执行\n",
    "    def call(self, x, is_training=1):\n",
    "        temp = self.Encoder0(x, training=is_training)\n",
    "        out = self.Encoder1(temp, training=is_training)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# 解码器\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, x_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.x_dim = x_dim\n",
    "\n",
    "        self.Decoder = tf.keras.Sequential([\n",
    "\n",
    "            layers.Dense(128),\n",
    "            layers.LeakyReLU(0.2),\n",
    "            layers.Reshape((8, 16)),\n",
    "\n",
    "            layers.Conv1D(32, 3, padding='same'),\n",
    "            \n",
    "            layers.Conv1D(16, 3),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Flatten(),\n",
    "            \n",
    "            layers.Dense(self.x_dim, activation='tanh'),\n",
    "        ])\n",
    "\n",
    "    def call(self, x, is_training=1):\n",
    "        out = self.Decoder(x, training=is_training)\n",
    "        return out\n",
    "\n",
    "\n",
    "def sample_z(mean, stddev):\n",
    "    \"\"\"\n",
    "    根据均值和标准差进行采样得到z\n",
    "    具体做法：先得到z的标准正态分布，再根据均值和标准差进行变换得到z\n",
    "    :param : mean : 均值\n",
    "    :param : stddev : 标准差\n",
    "    \"\"\"\n",
    "    # # 标准正态分布生成器\n",
    "    # std_normal_distribution_init = tf.random_normal_initializer(stddev=1.0) \n",
    "    # # 得到z的标准正态分布std_z\n",
    "    # std_z = std_normal_distribution_init(shape=mean.shape)\n",
    "    \n",
    "    # # 变换 \n",
    "    # z = mean + std_z * stddev\n",
    "\n",
    "    # return z\n",
    "\n",
    "    eps_init = tf.random_normal_initializer()\n",
    "    eps = eps_init(shape=mean.shape)\n",
    "\n",
    "    return mean + eps * tf.exp(stddev)\n",
    "\n",
    "\n",
    "def get_vae_prior_loss(mean, stddec):\n",
    "    \"\"\"\n",
    "    计算VAE的先验损失（Dkl）\n",
    "    :param : mean : 隐编码分布的均值\n",
    "    :param : stddec : 隐编码分布的标准差，改为标准差的对数\n",
    "\n",
    "    :return : vae_prior_loss : VAE的先验损失\n",
    "    \"\"\"\n",
    "    # vae_prior_loss = - tf.reduce_mean(0.5 * (tf.math.log(tf.square(stddec)) - tf.square(stddec) - tf.square(mean) + 1))\n",
    "    vae_prior_loss = - tf.reduce_mean(0.5 * (2 * stddec - tf.square(tf.exp(stddec)) - tf.square(mean) + 1))\n",
    "    return vae_prior_loss\n",
    "\n",
    "\n",
    "def get_vae_likelihood_loss(original_x, generated_x):\n",
    "    \"\"\"\n",
    "    计算VAE的似然损失\n",
    "    :param : original_x : 原样本\n",
    "    :param : generated_x : 经过VAE网络生成的的样本\n",
    "\n",
    "    :return : vae_likelihood_loss : VAE的似然损失\n",
    "    \"\"\"\n",
    "    vae_likelihood_loss = tf.reduce_mean(tf.square(original_x - generated_x))\n",
    "    return vae_likelihood_loss\n",
    "\n",
    "\n",
    "def get_vae_loss(mean, stddec, original_x, generated_x, alpha_1=1.5, alpha_2=0.1):\n",
    "    \"\"\"\n",
    "    计算VAE的损失\n",
    "    \n",
    "    :param : mean : 隐编码分布的均值\n",
    "    :param : stddec : 隐编码分布的标准差\n",
    "    :param : original_x : 原样本\n",
    "    :param : generated_x : 经过VAE网络生成的的样本\n",
    "    :param : alpha_1 :   default = 1.5\n",
    "    :param : alpha_2 :   default = 0.1\n",
    "\n",
    "    :return : loss : 计算vae损失\n",
    "    \"\"\"\n",
    "    vae_prior_loss = get_vae_prior_loss(mean=mean, stddec=stddec)\n",
    "    vae_likelihood_loss = get_vae_likelihood_loss(original_x=original_x, generated_x=generated_x)\n",
    "    \n",
    "    vae_loss = (alpha_1 * vae_prior_loss) + (alpha_2 * vae_likelihood_loss)\n",
    "    \n",
    "    return vae_loss\n",
    "\n",
    "def init_net(x_dim, enc_dim):\n",
    "    \"\"\"\n",
    "    初始化SMSG_PRO的网络结构\n",
    "    enc    dec\n",
    "    map_net\n",
    "    dis\n",
    "    \n",
    "    :param : x_dim : 样本的维度\n",
    "    :param : enc_dim : 编码器编码的维度\n",
    "\n",
    "    :return : smsg_net_dict = {'enc':enc, 'dec':dec, 'map_net':map_net, 'dis':dis}\n",
    "    \"\"\"\n",
    "    enc = Encoder(enc_dim)  # 编码器\n",
    "    dec = Decoder(x_dim)  # 解码器\n",
    "    map_net = Map_Net(enc_dim)  # 映射网络\n",
    "    dis = Discriminator(x_dim)  # 对样本进行判别的判别器\n",
    "    dis_map = Discriminator(x_dim=enc_dim)  # 对隐编码进行判别的判别器\n",
    "\n",
    "    smsg_net_dict = {'enc': enc, 'dec': dec, 'map_net': map_net, 'dis': dis, 'dis_map': dis_map}\n",
    "    return smsg_net_dict\n",
    "\n",
    "\n",
    "def init_optimizers(learning_rate=2e-4, beta_1=0.5):\n",
    "    \"\"\"\n",
    "    初始化上述网络的优化器\n",
    "    网络优化策略设置，优化器为Adam(学习率为2e-4，beta_1为0.5)\n",
    "    :param : learning_rate default = 2e-4\n",
    "    :param : beta_1 default = 0.5\n",
    "\n",
    "    :return : optimizers_dict = {'optimizer_enc':optimizer_enc, 'optimizer_dec':optimizer_dec,\n",
    "                                 'optimizer_map_net':optimizer_map_net, 'optimizer_dis':optimizer_dis, }\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    optimizer_enc = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1)\n",
    "    optimizer_dec = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1)\n",
    "    optimizer_map_net = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1)\n",
    "    optimizer_dis = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1)\n",
    "    optimizer_dis_map = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1)\n",
    "\n",
    "    optimizers_dict = {'optimizer_enc': optimizer_enc, 'optimizer_dec': optimizer_dec,\n",
    "                       'optimizer_map_net': optimizer_map_net, 'optimizer_dis': optimizer_dis,\n",
    "                       'optimizer_dis_map': optimizer_dis_map}\n",
    "\n",
    "    return optimizers_dict\n",
    "\n",
    "\n",
    "def train(smsg_net_dict, optimizers_dict, train_batch, val_batch, vae_loss_paramaters_dict, columns, epochs=300):\n",
    "    \"\"\"\n",
    "    一次交叉验证的训练过程，同时将每训练一次batch的相关的损失写入到文件中\n",
    "    :param : smsg_net_dict : SMSG_PRO网络结构字典\n",
    "    :param : optimizers_dict : 所有网络优化器字典\n",
    "    :param : train_batch : 一次交叉验证中训练集的batch，包括多数类样本和少数类样本的batches\n",
    "    :param : val_batch : 一次交叉验证中验证集的batch，包括多数类样本和少数类样本的batches\n",
    "    :param : vae_loss_paramaters_dict : 计算VAE损失的参数的字典 {'alpha_1':alpha_1_value, 'alpha_2':alpha_2_value}\n",
    "    :param : columns : 列名，用于构造新样本时df的构造，检验后验崩塌\n",
    "    :param : epochs : 训练轮数\n",
    "    \n",
    "    :return : optimized_smsg_net_dict : 优化过的SMSG的网络结构\n",
    "    \"\"\"\n",
    "\n",
    "    # 初始化损失的pandas.DataFrame\n",
    "    # loss_df = pd.DataFrame(columns = ['epoch', 'enc0_loss', 'dec0_loss', 'enc1_loss', 'dec1_loss', 'dis0_loss', 'dis1_loss'])\n",
    "\n",
    "    # 获取SMSG_PRO中所有的网络以及对应的优化器\n",
    "    enc = smsg_net_dict.get('enc')\n",
    "    dec = smsg_net_dict.get('dec')\n",
    "    map_net = smsg_net_dict.get('map_net')\n",
    "    dis = smsg_net_dict.get('dis')\n",
    "    dis_map = smsg_net_dict.get('dis_map')\n",
    "    optimizer_enc = optimizers_dict.get('optimizer_enc')\n",
    "    optimizer_dec = optimizers_dict.get('optimizer_dec')\n",
    "    optimizer_map_net = optimizers_dict.get('optimizer_map_net')\n",
    "    optimizer_dis = optimizers_dict.get('optimizer_dis')\n",
    "    optimizer_dis_map = optimizers_dict.get('optimizer_dis_map')\n",
    "\n",
    "    # 获取计算VAE损失的参数alpha_1和alpha_2\n",
    "    alpha_1 = vae_loss_paramaters_dict.get('alpha_1')\n",
    "    alpha_2 = vae_loss_paramaters_dict.get('alpha_2')\n",
    "\n",
    "    val_performance_list = []\n",
    "    optimized_smsg_net_dict_list = []\n",
    "\n",
    "    # 综合训练\n",
    "    for epoch in range(epochs):\n",
    "        # 根据train_batch获取每一次训练的多数类样本batch和少数类样本batch\n",
    "        for data_0_batch, data_1_batch in zip(train_batch[0], train_batch[1]):\n",
    "            x_0, y_0 = data_0_batch\n",
    "            x_1, y_1 = data_1_batch\n",
    "\n",
    "            # 类型转换，将x_0, x_1中数据类型转为tf.float32\n",
    "            x_0 = tf.cast(x_0, dtype=tf.float32)\n",
    "            x_1 = tf.cast(x_1, dtype=tf.float32)\n",
    "\n",
    "            # 使用自动微分机制进行训练\n",
    "            with tf.GradientTape() as enc_tape, tf.GradientTape() as dec_tape, tf.GradientTape() as map_net_tape, tf.GradientTape() as dis_tape, tf.GradientTape() as dis_map_tape:\n",
    "                # 多数类样本重构\n",
    "                mean_and_stddec_0 = enc(x_0)  # 使用编码器对多数类样本进行编码\n",
    "                mean_0, stddec_0 = tf.split(mean_and_stddec_0, 2, 1)  # 将1x128拆分为2x64，获取分布的两个参数\n",
    "                z_0 = sample_z(mean_0, stddec_0)  # 根据分布的两个参数得到分布然后采样取值\n",
    "                x_0_0 = dec(z_0)  # 使用解码器0生成多数类样本，即由多数类样本重构得到的多数类样本\n",
    "\n",
    "                # 多数类样本迁移得到迁移少数类样本\n",
    "                mean_and_stddec_0_1 = map_net(mean_and_stddec_0)  # 多数类样本的编码经过映射网络得到少数类样本的迁移编码\n",
    "                mean_0_1, stddec_0_1 = tf.split(mean_and_stddec_0_1, 2, 1)  # 获取分布的两个参数\n",
    "                z_0_1 = sample_z(mean_0_1, stddec_0_1)  # 期望与z_1一致\n",
    "                x_0_1 = dec(z_0_1)  # 使用迁移编码经过解码器得到的迁移少数类样本\n",
    "\n",
    "                # 少数类样本重构\n",
    "                mean_and_stddec_1 = enc(x_1)  # 使用编码器对少数类样本进行编码\n",
    "                mean_1, stddec_1 = tf.split(mean_and_stddec_1, 2, 1)  # 将1x128拆分为2x64，获取分布的两个参数\n",
    "                z_1 = sample_z(mean_1, stddec_1)  # 根据分布的两个参数得到分布然后采样取值\n",
    "                x_1_1 = dec(z_1)  # 由少数类样本重构得到的少数类样本\n",
    "\n",
    "                # 原样本，重构样本以及迁移样本通过判别器\n",
    "                dis_output_1 = dis(x=x_1)\n",
    "                dis_output_1_1 = dis(x=x_1_1)\n",
    "                dis_output_0_1 = dis(x=x_0_1)\n",
    "\n",
    "                # 不同编码通过编码的判别器\n",
    "                dis_map_output_1 = dis_map(x=z_1)\n",
    "                dis_map_output_0_1 = dis_map(x=z_0_1)\n",
    "\n",
    "                # VAE的损失（alpha_1*先验+alpha_2*似然）\n",
    "                vae_0_0_likelihood_loss = get_vae_likelihood_loss(original_x=x_0, generated_x=x_0_0)\n",
    "                vae_1_1_likelihood_loss = get_vae_likelihood_loss(original_x=x_1, generated_x=x_1_1)\n",
    "                vae_0_0_loss = get_vae_loss(mean=mean_0, stddec=stddec_0, original_x=x_0, generated_x=x_0_0,\n",
    "                                            alpha_1=alpha_1, alpha_2=alpha_2)\n",
    "                vae_1_1_loss = get_vae_loss(mean=mean_1, stddec=stddec_1, original_x=x_1, generated_x=x_1_1,\n",
    "                                            alpha_1=alpha_1, alpha_2=alpha_2)\n",
    "\n",
    "                # 解码器生成迁移样本的损失\n",
    "                dec_gen_migration_loss = - tf.math.log(tf.reduce_mean(dis_output_0_1))\n",
    "\n",
    "                # 多数类迁移少数类编码与少数类编码的一致性损失   # 需要求一个方向上的均值\n",
    "                code_consistency_loss = tf.reduce_mean(\n",
    "                    tf.square(tf.reduce_mean(z_1, axis=0) - tf.reduce_mean(z_0_1, axis=0)))\n",
    "\n",
    "                # 欧氏距离约束    # 需要求一个方向上的均值\n",
    "                loss_distance_0_1_to_1 = tf.reduce_mean(\n",
    "                    tf.square(tf.reduce_mean(x_0_1, axis=0) - tf.reduce_mean(x_1, axis=0)))\n",
    "                loss_distance_0_1_to_0 = tf.reduce_mean(\n",
    "                    tf.square(tf.reduce_mean(x_0_1, axis=0) - tf.reduce_mean(x_0, axis=0)))\n",
    "                loss_distance = 2 * loss_distance_0_1_to_1 + 1 * loss_distance_0_1_to_0\n",
    "\n",
    "                # 映射网络转换编码的损失\n",
    "                loss_map_enc = - tf.math.log(tf.reduce_mean(dis_map_output_0_1))\n",
    "\n",
    "                # 需要得到的损失\n",
    "                enc_loss = vae_0_0_loss + vae_1_1_loss\n",
    "                dec_loss = vae_0_0_likelihood_loss + vae_1_1_likelihood_loss + dec_gen_migration_loss + loss_distance\n",
    "                map_net_loss = code_consistency_loss + loss_distance + loss_map_enc\n",
    "                # dis_loss = 2.0 * tf.reduce_mean(dis_output_0_1) - tf.reduce_mean(dis_output_1_1) - tf.reduce_mean(dis_output_1)\n",
    "\n",
    "                dis_loss = - (\n",
    "                            tf.math.log(tf.reduce_mean(dis_output_1)) + tf.math.log(1 - tf.reduce_mean(dis_output_0_1)))\n",
    "                dis_map_loss = - (tf.math.log(tf.reduce_mean(dis_map_output_1)) + tf.math.log(\n",
    "                    1 - tf.reduce_mean(dis_map_output_0_1)))\n",
    "                # \n",
    "                # loss = enc_loss + dec_gen_migration_loss + loss_distance + 1.2 * map_net_loss + dis_loss\n",
    "\n",
    "                '''\n",
    "                    # 将上述损失添加到loss_df中\n",
    "                    # 需要把tensor数据类型转为float\n",
    "                    # loss_df = loss_df.append([{'epoch':epoch, \n",
    "                    #                             'enc_loss':float(enc_loss.numpy()), \n",
    "                    #                             'dec_loss':float(dec_loss.numpy()), \n",
    "                    #                             'map_net_loss':float(map_net_loss.numpy()), \n",
    "                    #                             'dis_loss':float(dis_loss.numpy())}], ignore_index=True)\n",
    "                '''\n",
    "            # 计算梯度，优化编码器，解码器以及判别器\n",
    "            grads = enc_tape.gradient(enc_loss, enc.trainable_variables)\n",
    "            optimizer_enc.apply_gradients(zip(grads, enc.trainable_variables))\n",
    "\n",
    "            grads = dec_tape.gradient(dec_loss, dec.trainable_variables)\n",
    "            optimizer_dec.apply_gradients(zip(grads, dec.trainable_variables))\n",
    "\n",
    "            grads = map_net_tape.gradient(map_net_loss, map_net.trainable_variables)\n",
    "            optimizer_map_net.apply_gradients(zip(grads, map_net.trainable_variables))\n",
    "\n",
    "            grads = dis_tape.gradient(dis_loss, dis.trainable_variables)\n",
    "            optimizer_dis.apply_gradients(zip(grads, dis.trainable_variables))\n",
    "\n",
    "            grads = dis_map_tape.gradient(dis_map_loss, dis_map.trainable_variables)\n",
    "            optimizer_dis_map.apply_gradients(zip(grads, dis_map.trainable_variables))\n",
    "\n",
    "        # 保存每一个epoch后的网络结构，并用测试验证集的性能指标\n",
    "        optimized_smsg_net_dict = {'enc': enc, 'dec': dec, 'map_net': map_net, 'dis': dis}\n",
    "        optimized_smsg_net_dict_list.append(optimized_smsg_net_dict)\n",
    "        balanced_train_dataset_df = generate_new_1_sample(optimized_smsg_net_dict, train_batch, columns=columns)\n",
    "        val_dataset_df = batch_list_to_df(val_batch, columns=columns)\n",
    "\n",
    "        val_dataset_performance_sum = np.array(\n",
    "            get_three_methods_performance_df(balanced_train_dataset_df, val_dataset_df).iloc[:, 1:]).astype(\n",
    "            'double').sum()\n",
    "        # print(val_dataset_performance_sum)\n",
    "        val_performance_list.append(val_dataset_performance_sum)\n",
    "\n",
    "        # 每100个epoch\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            # if True:\n",
    "            #     # 每100个epoch绘制一下图像，检验有没有后验崩塌的现象存在\n",
    "            #     optimized_smsg_net_dict = {'enc':enc, 'dec':dec, 'map_net':map_net, 'dis':dis}\n",
    "            #     balanced_train_dataset_df = generate_new_1_sample(optimized_smsg_net_dict, train_batch, columns=columns)\n",
    "            #     x1, y1 = balanced_train_dataset_df.iloc[:, :-1].astype('float'), balanced_train_dataset_df.iloc[:, -1].astype('int')\n",
    "            #     x1 = pca.transform(x1)\n",
    "            #     fig1 = sns.stripplot(x=x1[:, 0], y=x1[:, 1], hue=y1)\n",
    "            #     scatter_fig1 = fig1.get_figure()\n",
    "            #     scatter_fig1.savefig('/home/lqw/testone/my_code_pro_pro_pro/smsg/images/' + 'balanced_pima_' + str(index + 1) + '_' + str(epoch + 1) + '.png')\n",
    "            #     fig1.clear()\n",
    "\n",
    "            print('epoch' + str(epoch + 1) + '训练完成！', end='\\t')\n",
    "\n",
    "    # 将每个epoch训练得到的loss_df写入到文件中\n",
    "    # loss_df.to_csv('smsg/loss_logger/train_loss.csv', index=False, header=True, sep=',')\n",
    "\n",
    "    # 优化过的SMSG的网络结构\n",
    "    # optimized_smsg_net_dict = {'enc':enc, 'dec':dec, 'map_net':map_net, 'dis':dis}\n",
    "    # 使用300个epoch中对验证集效果最好的一组网络结构作为最终的网络结构\n",
    "    max_index = val_performance_list.index(max(val_performance_list))\n",
    "    print('最好的epoch是：' + str(max_index))\n",
    "    optimized_smsg_net_dict = optimized_smsg_net_dict_list[max_index]\n",
    "\n",
    "    return optimized_smsg_net_dict\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate new samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集预处理成功\t\t取出训练batches成功\t\t初始化网络和优化器成功\t"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer \"reshape_4\" \"                 f\"(type Reshape).\n\n{{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:CPU:0}} Input to reshape is a tensor with 1188 values, but the requested shape has 190080 [Op:Reshape]\n\nCall arguments received by layer \"reshape_4\" \"                 f\"(type Reshape):\n  • inputs=tf.Tensor(shape=(36, 33), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20260\\3340324623.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\t初始化网络和优化器成功'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# 进行一次交叉验证中的训练，得到优化后的SMSG的网络结构\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0moptimized_smsg_net_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msmsg_net_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msmsg_net_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizers_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizers_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_batch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvae_loss_paramaters_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvae_loss_paramaters_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\t网络优化成功'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# 从优化好的网络中生成新的少数类样本得到平衡的数据集df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20260\\1376519459.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(smsg_net_dict, optimizers_dict, train_batch, val_batch, vae_loss_paramaters_dict, columns, epochs)\u001b[0m\n\u001b[0;32m    508\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0menc_tape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdec_tape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmap_net_tape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdis_tape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdis_map_tape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m                 \u001b[1;31m# 多数类样本重构\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 510\u001b[1;33m                 \u001b[0mmean_and_stddec_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_0\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 使用编码器对多数类样本进行编码\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    511\u001b[0m                 \u001b[0mmean_0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstddec_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_and_stddec_0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 将1x128拆分为2x64，获取分布的两个参数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m                 \u001b[0mz_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_z\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstddec_0\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 根据分布的两个参数得到分布然后采样取值\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\zxj\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20260\\1376519459.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, x, is_training)\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[1;31m# 具体执行\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m         \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEncoder0\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_training\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEncoder1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_training\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer \"reshape_4\" \"                 f\"(type Reshape).\n\n{{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:CPU:0}} Input to reshape is a tensor with 1188 values, but the requested shape has 190080 [Op:Reshape]\n\nCall arguments received by layer \"reshape_4\" \"                 f\"(type Reshape):\n  • inputs=tf.Tensor(shape=(36, 33), dtype=float32)"
     ]
    }
   ],
   "source": [
    "# 五次交叉验证\n",
    "for i in range(1, 6):\n",
    "    dataset_path = project_path + 'eeg/split/' + str(i) + '/train/'\n",
    "    if not os.path.exists(dataset_path):\n",
    "        os.makedirs(dataset_path)\n",
    "    balanced_dataset_path = project_path + 'eeg/split/' + str(i) + '/balanced_train/'\n",
    "    if not os.path.exists(balanced_dataset_path):\n",
    "        os.makedirs(balanced_dataset_path)    \n",
    "    # 获取所有的数据集名称的列表\n",
    "    keel_dataset_name_list = os.listdir(dataset_path)\n",
    "    # 对每一个数据集进行处理\n",
    "    for keel_dataset_name in keel_dataset_name_list:\n",
    "        # 加载数据集\n",
    "        my_df = load_keel_dataset(dataset_path, keel_dataset_name)\n",
    "        # 数据集预处理\n",
    "        my_df_after_preprocess = keel_dataset_preprocess(my_df)\n",
    "        print('数据集预处理成功', end='\\t')\n",
    "        # 获取训练样本x的特征维度，方便后续对网络进行初始化\n",
    "        x_dim = my_df_after_preprocess.shape[1] - 1\n",
    "        columns = my_df_after_preprocess.columns\n",
    "        # 获取K次交叉验证中训练集和测试集的batches列表\n",
    "        batch_list = get_keel_dataset_batch_list(my_df_after_preprocess, batch_size=batch_size, my_random_state=2022)\n",
    "        train_batch = batch_list[0]\n",
    "        val_batch = batch_list[1]\n",
    "        print('\\t取出训练batches成功', end='\\t')\n",
    "        # 初始化SMSG网络及其对应的优化器\n",
    "        smsg_net_dict = init_net(x_dim=x_dim, enc_dim=enc_dim)\n",
    "        optimizers_dict = init_optimizers(learning_rate=2e-4, beta_1=0.5)\n",
    "        print('\\t初始化网络和优化器成功', end='\\t')\n",
    "        # 进行一次交叉验证中的训练，得到优化后的SMSG的网络结构\n",
    "        optimized_smsg_net_dict = train(smsg_net_dict=smsg_net_dict, optimizers_dict=optimizers_dict, train_batch=train_batch, val_batch=val_batch, vae_loss_paramaters_dict=vae_loss_paramaters_dict, columns=columns, epochs=epochs)\n",
    "        print('\\t网络优化成功', end='\\t')\n",
    "        # 从优化好的网络中生成新的少数类样本得到平衡的数据集df\n",
    "        balanced_train_dataset_df = generate_new_1_sample(optimized_smsg_net_dict, train_batch, columns=columns)\n",
    "        balanced_train_dataset_df.get('rf').to_csv(balanced_dataset_path + keel_dataset_name[:-4] + '_rf.csv', index=False)\n",
    "        balanced_train_dataset_df.get('lr').to_csv(balanced_dataset_path + keel_dataset_name[:-4] + '_lr.csv', index=False)\n",
    "        balanced_train_dataset_df.get('svm').to_csv(balanced_dataset_path + keel_dataset_name[:-4] + '_svm.csv', index=False)\n",
    "\n",
    "        print('\\t生成新的少数类样本成功', end='\\t')\n",
    "        print(keel_dataset_name + \"结束！！！\")\n",
    "\n",
    "print('jieshu')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zxj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
